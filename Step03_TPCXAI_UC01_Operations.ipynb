{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 0.0.2  Updated date: 07/05/2024\n",
    "Conda Environment : py-snowpark_df_ml_fs-1.15.0_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Snowflake Feature Store -  Customer Segmentation\n",
    "\n",
    "The Customer segmentation (UC01) use case is designed to emulate a data science pipeline to find clusters of customers based  on  aggregate  features where  the  customers  are  grouped  based  on  their  spending behavior. <br>\n",
    "\n",
    "It  involves  creating subgroups of customers based on similar traits. <br>\n",
    "\n",
    "The input in this use case consists of order and return transaction data from a retail business. <br>\n",
    "\n",
    "The use case uses Tables Customer, Order, Lineitem and Order_returns. <br>\n",
    "\n",
    "K-means  clustering  algorithm  is  used  to  derive  the  optimum  number  of  clusters  and  understand  the  underlying customer segments based on the data provided. <br>\n",
    "Clustering is an unsupervised machine learning technique, where there are no defined dependent and independent variables, i.e. the training samples are unlabeled. <br>\n",
    "The pattern in the data is used to identify and group similar observations. <br>\n",
    "\n",
    "We will use the Use-Case to show how Snowflake Feature Store (and Model Registry) can be used to maintain & store features, retrieve them for training and perform micro-batch inference.\n",
    "\n",
    "In the development (TRAINING) enviroment we will \n",
    "- create FeatureViews in the Feature Store that maintain the required customer-behaviour features.\n",
    "- use these Features to train a model, and save the model in the Snowflake model-registry.\n",
    "- plot the clusters for the trained model to visually verify. \n",
    "\n",
    "In the production (SERVING) environment we will\n",
    "- re-create the FeatureViews on production data\n",
    "- generate an Inference FeatureView that uses the saved model to perform incremental inference\n",
    "\n",
    "# Model Operationalisation in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import json\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import datetime\n",
    "import ast      \n",
    "import sqlglot\n",
    "import sqlglot.optimizer.optimizer\n",
    "\n",
    "# SNOWFLAKE\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec\n",
    "#from snowflake.snowpark import Analytics\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode)\n",
    "\n",
    "# Snowflake Model Registry\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.utils import connection_params\n",
    "from snowflake.ml._internal.utils import identifier  \n",
    "\n",
    "\n",
    "# COMMON FUNCTIONS\n",
    "from useful_fns import check_and_update, formatSQL, create_ModelRegistry, create_FeatureStore \n",
    "\n",
    "#### Use-Case 01 - Specific Packages\n",
    "# K-Means clustering\n",
    "#from sklearn.pipeline import Pipeline as skl_Pipeline\n",
    "from snowflake.ml.modeling.pipeline import Pipeline as sml_Pipeline\n",
    "#from sklearn.preprocessing import MinMaxScaler as skl_MinMaxScaler\n",
    "from snowflake.ml.modeling.preprocessing import MinMaxScaler as sml_MinMaxScaler\n",
    "#from sklearn.cluster import KMeans as skl_KMeans\n",
    "from snowflake.ml.modeling.cluster import KMeans as sml_KMeans\n",
    "\n",
    "# Feature Engineering Functions\n",
    "from feature_engineering_fns import uc01_load_data, uc01_pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Snowflake connection and database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Scale Factor\n",
    "scale_factor               = 'SF0001'\n",
    "\n",
    "# Roles\n",
    "fs_qs_role                 = 'RAKESHGADIPARTHI'\n",
    "\n",
    "# Database\n",
    "tpcxai_database_base       = f'TPCXAI_{scale_factor}_QUICKSTART'\n",
    "tpcxai_database            = f'{tpcxai_database_base}_INC'\n",
    "\n",
    "# Schemas\n",
    "tpcxai_training_schema     = 'TRAINING'\n",
    "tpcxai_scoring_schema      = 'SCORING'\n",
    "tpcxai_serving_schema      = 'SERVING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We point the `tpcxai_schema` variable to our `SERVING` schema, and this one change allows us to recreate the model development pipeline in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Schema (Environment)\n",
    "tpcxai_schema = tpcxai_serving_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : SF$SERVICE$mfU6u2k_D-0UpGPF4MnAMg\n",
      "Role                        : \"RAKESHGADIPARTHI\"\n",
      "Database                    : \"TPCXAI_SF0001_QUICKSTART_INC\"\n",
      "Schema                      : \"SERVING\"\n",
      "Warehouse                   : \"FOSFOR_INSIGHT_WH\"\n",
      "Snowflake version           : 8.28.0\n",
      "Snowpark for Python version : 1.20.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "connection_parameters = connection_params.SnowflakeLoginOptions(\"ak32940\")\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Set  Environment\n",
    "session.sql(f'''use database {tpcxai_database}''').collect()\n",
    "session.sql(f'''use schema {tpcxai_schema}''').collect()\n",
    "session.sql(f'''use role {fs_qs_role}''').collect()\n",
    "\n",
    "# Create a Warehouse\n",
    "#warehouse_sz = 'MEDIUM'\n",
    "warehouse_env = f'FOSFOR_INSIGHT_WH'\n",
    "session.sql(f'''use warehouse {warehouse_env}''').collect()\n",
    "#session.sql(f'''alter warehouse {warehouse_env} set warehouse_size = {warehouse_sz}''').collect()\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print(f'User                        : {snowflake_environment[0][0]}')\n",
    "print(f'Role                        : {session.get_current_role()}')\n",
    "print(f'Database                    : {session.get_current_database()}')\n",
    "print(f'Schema                      : {session.get_current_schema()}')\n",
    "print(f'Warehouse                   : {session.get_current_warehouse()}')\n",
    "print(f'Snowflake version           : {snowflake_environment[0][1]}')\n",
    "print(f'Snowpark for Python version : {snowpark_version[0]}.{snowpark_version[1]}.{snowpark_version[2]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL OPERATIONALISATION\n",
    "* Recreate production Entity, FeatureViews in Production FeatureStore\n",
    "* Reuse the model fitted in development/training\n",
    "* Create new Inference FeatureView for incremental model-inference\n",
    "\n",
    "#### Setup Production Feature Store and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Registry (_MODEL_REGISTRY) already exists\n",
      "Feature Store (_SERVING_FEATURE_STORE) created\n",
      "--- Created Data References ---\n"
     ]
    }
   ],
   "source": [
    "# Create/Reference Snowflake Model Registry - Common across Environments\n",
    "mr = create_ModelRegistry(session, tpcxai_database, '_MODEL_REGISTRY')\n",
    "\n",
    "# Create/Reference Snowflake Feature Store for Training (Development) Environment\n",
    "fs = create_FeatureStore(session, tpcxai_database, f'''_{tpcxai_schema}_FEATURE_STORE''', warehouse_env)\n",
    "\n",
    "### Reference Data to Snowflake Dataframe Objects\n",
    "# Tables\n",
    "customer_tbl               = '.'.join([tpcxai_database, tpcxai_schema,'CUSTOMER'])\n",
    "line_item_tbl              = '.'.join([tpcxai_database, tpcxai_schema,'LINEITEM'])\n",
    "order_tbl                  = '.'.join([tpcxai_database, tpcxai_schema,'ORDERS'])\n",
    "order_returns_tbl          = '.'.join([tpcxai_database, tpcxai_schema,'ORDER_RETURNS'])\n",
    "\n",
    "# Snowpark Dataframe\n",
    "customer_sdf               = session.table(customer_tbl)\n",
    "line_item_sdf              = session.table(line_item_tbl)\n",
    "order_sdf                  = session.table(order_tbl)\n",
    "order_returns_sdf          = session.table(order_returns_tbl)\n",
    "print('''--- Created Data References ---''')\n",
    "\n",
    "# Model Name\n",
    "model_name = \"UC01_SNOWFLAKEML_KMEANS_MODEL\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now rerun the exact same code that we lifted from our Development (TRAINING) process to recreate the Feature Engineering pipelines in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Created CUSTOMER Entity ---\n",
      "--- Created Source Data ---\n",
      "--- Created Preprocessed Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/snowflake/ml/feature_store/feature_store.py:1066: UserWarning: Your pipeline won't be incrementally refreshed due to: \"This dynamic table contains a complex query. Refresh mode has been set to FULL. If you wish to override this automatic choice, please re-create the dynamic table and specify REFRESH_MODE=INCREMENTAL. For best results, we recommend reading https://docs.snowflake.com/user-guide/dynamic-table-performance-guide before setting the refresh mode to INCREMENTAL.\". It will likely incurr higher cost.\n",
      "  self._check_dynamic_table_refresh_mode(feature_view_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature View : FV_UC01_PREPROCESS_V_1 created in SERVING\n",
      "---            DONE               ---\n"
     ]
    }
   ],
   "source": [
    "### CUSTOMER Entity\n",
    "if \"CUSTOMER\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n",
    "    customer_entity = Entity(name=\"CUSTOMER\", join_keys=[\"O_CUSTOMER_SK\"],desc=\"Primary Key for CUSTOMER\")\n",
    "    fs.register_entity(customer_entity)\n",
    "else:\n",
    "    customer_entity = fs.get_entity(\"CUSTOMER\")\n",
    "print('''--- Created CUSTOMER Entity ---''')\n",
    "\n",
    "### Create & Load Source Data\n",
    "raw_data = uc01_load_data(order_sdf, line_item_sdf, order_returns_sdf)\n",
    "rd_sql = formatSQL(raw_data.queries['queries'][0], True)\n",
    "print('''--- Created Source Data ---''')\n",
    "\n",
    "### Create & Run Preprocessing Function \n",
    "preprocessed_data = uc01_pre_process(raw_data)\n",
    "ppd_sql = formatSQL(preprocessed_data.queries['queries'][0], True)\n",
    "print('''--- Created Preprocessed Data ---''')\n",
    "\n",
    "### Create Preprocessing FeatureView from Preprocess Dataframe (SQL)\n",
    "ppd_fv_name = \"FV_UC01_PREPROCESS\"\n",
    "ppd_fv_version = \"V_1\"\n",
    "# Define descriptions for the FeatureView's Features.  These will be added as comments to the database object\n",
    "preprocess_features_desc = { \"FREQUENCY\":\"Average yearly order frequency\",\n",
    "                             \"RETURN_RATIO\":\"Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value\" }\n",
    "# Create Inference Feature View\n",
    "try:\n",
    "    # If FeatureView already exists just return the reference to it\n",
    "    fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\n",
    "except:\n",
    "    # Create the FeatureView instance\n",
    "    fv_uc01_preprocess_instance = FeatureView(\n",
    "        name=ppd_fv_name, \n",
    "        entities=[customer_entity], \n",
    "        #feature_df=preprocessed_data,      # <- We can use the snowpark dataframe as-is from our Python\n",
    "        feature_df=session.sql(ppd_sql),    # <- Or we can use SQL, in this case linted from the dataframe generated SQL to make more human readable\n",
    "        timestamp_col=\"LATEST_ORDER_DATE\",\n",
    "        refresh_freq=\"60 minute\",           # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "        desc=\"Features to support Use Case 01\").attach_feature_desc(preprocess_features_desc)\n",
    "\n",
    "    # Register the FeatureView instance.  Creates  object in Snowflake\n",
    "    fv_uc01_preprocess = fs.register_feature_view(\n",
    "        feature_view=fv_uc01_preprocess_instance, \n",
    "        version=ppd_fv_version, \n",
    "        block=True\n",
    "    )\n",
    "    print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} created in {tpcxai_schema}\")   \n",
    "else:\n",
    "    print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} already created in {tpcxai_schema}\")\n",
    "\n",
    "print('''---            DONE               ---''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Scheduled Inference Pipeline\n",
    "\n",
    "We now recreate our model inference process that will\n",
    "- retrieve the latest version of the model from the Model Registry.\n",
    "- read features from our feature pipeline (fv_uc01_preprocess featureview)\n",
    "- pass features & model into inference function (uc01_serve) and return inference dataframe\n",
    "- use inference dataframe to define a new FeatureView to maintain inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def uc01_serve(featurevector, km4_purchases) -> DataFrame:\n",
    "    return km4_purchases.run(featurevector, function_name=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "|\"O_CUSTOMER_SK\"  |\"FREQUENCY\"  |\"RETURN_RATIO\"  |\"LATEST_ORDER_DATE\"  |\n",
      "------------------------------------------------------------------------\n",
      "|                 |             |                |                     |\n",
      "------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Inference Dataframe that reads from our feature-engineering pipeline\n",
    "inference_input_sdf = fs.read_feature_view(fv_uc01_preprocess)\n",
    "inference_input_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "|\"O_CUSTOMER_SK\"  |\"FREQUENCY\"  |\"RETURN_RATIO\"  |\"LATEST_ORDER_DATE\"  |\"RETURN_RATIO_MMS\"  |\"FREQUENCY_MMS\"  |\"CLUSTER\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "|                 |             |                |                     |                    |                 |           |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Inference process\n",
    "inference_result_sdf = uc01_serve(inference_input_sdf, mv)\n",
    "inference_result_sdf.sort(F.col('LATEST_ORDER_DATE').desc(), F.col('O_CUSTOMER_SK')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the SQL output below how our model is packaged and called from SQL `MODEL_VERSION_ALIAS!PREDICT(RETURN_RATIO, FREQUENCY) AS TMP_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"O_CUSTOMER_SK\", \"FREQUENCY\", \"RETURN_RATIO\", \"LATEST_ORDER_DATE\",  CAST (\"TMP_RESULT\"['RETURN_RATIO_MMS'] AS DOUBLE) AS \"RETURN_RATIO_MMS\",  CAST (\"TMP_RESULT\"['FREQUENCY_MMS'] AS DOUBLE) AS \"FREQUENCY_MMS\",  CAST (\"TMP_RESULT\"['CLUSTER'] AS BIGINT) AS \"CLUSTER\" FROM (WITH SNOWPARK_ML_MODEL_INFERENCE_INPUT AS (SELECT * FROM TPCXAI_SF0001_QUICKSTART_INC._SERVING_FEATURE_STORE.FV_UC01_PREPROCESS$V_1),MODEL_VERSION_ALIAS AS MODEL TPCXAI_SF0001_QUICKSTART_INC._MODEL_REGISTRY.UC01_SNOWFLAKEML_KMEANS_MODEL VERSION V_1\n",
      "                SELECT *,\n",
      "                    MODEL_VERSION_ALIAS!PREDICT(RETURN_RATIO, FREQUENCY) AS TMP_RESULT\n",
      "                FROM SNOWPARK_ML_MODEL_INFERENCE_INPUT)\n"
     ]
    }
   ],
   "source": [
    "ind_sql = inference_result_sdf.queries['queries'][0]\n",
    "ind_fmtd_sql = os.linesep.join(ind_sql.split(os.linesep)[:1000])\n",
    "print(ind_fmtd_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Register Inference-FeatureView to run scheduled Inference\n",
    "\n",
    "We can now define a new Inference Feature View using our Spine and Dataframe reading from our Feature Engineering pipeline.  The FeatureView when created as a Dynamic Table will run to the required refresh_freq and automatically perform incremental inference on new data that arrives through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Feature View : fv_uc01_inference_result_V_2 created\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                    |\"VERSION\"  |\"DATABASE_NAME\"               |\"SCHEMA_NAME\"           |\"CREATED_ON\"                |\"OWNER\"           |\"DESC\"                                              |\"ENTITIES\"    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|FV_UC01_INFERENCE_RESULT  |V_2        |TPCXAI_SF0001_QUICKSTART_INC  |_SERVING_FEATURE_STORE  |2024-08-02 08:43:36.423000  |RAKESHGADIPARTHI  |Inference Result from kmeans model for Use Case 01  |[             |\n",
      "|                          |           |                              |                        |                            |                  |                                                    |  \"CUSTOMER\"  |\n",
      "|                          |           |                              |                        |                            |                  |                                                    |]             |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create & Register Inference-FeatureView to run scheduled Inference\n",
    "inf_fvname = \"FV_UC01_INFERENCE_RESULT\"\n",
    "inf_fv_version = \"V_2\"\n",
    "\n",
    "inference_features_desc = { \"FREQUENCY\":\"Average yearly order frequency\",\n",
    "                              \"RETURN_RATIO\":\"Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value\", \n",
    "                              \"RETURN_RATIO_MMS\":f\"Min/Max Scaled version of RETURN_RATIO using Model Registry ({tpcxai_database}_MODEL_REGISTRY) Model ({mv.model_name}) Model-Version({mv.version_name}) Model Comment ({mv.comment})\",\n",
    "                              \"FREQUENCY_MMS\":f\"Min/Max Scaled version of FREQUENCY using Model Registry ({tpcxai_database}_MODEL_REGISTRY) Model ({mv.model_name}) Model-Version({mv.version_name})  Model Comment ({mv.comment}\",\n",
    "                              \"CLUSTER\":f\"Kmeans Cluster for Customer Clustering Model (UC01) using Model Registry ({tpcxai_database}_MODEL_REGISTRY) Model ({mv.model_name}) Model-Version({mv.version_name})  Model Comment ({mv.comment}\"}\n",
    "\n",
    "try:\n",
    "   fv_uc01_inference_result = fs.get_feature_view(name= inf_fvname, version= inf_fv_version)\n",
    "except:\n",
    "   fv_uc01_inference_result = FeatureView(\n",
    "         name= inf_fvname, \n",
    "         entities=[customer_entity], \n",
    "         feature_df=inference_result_sdf,\n",
    "         ## refresh_freq=\"60 minute\",\n",
    "         desc=\"Inference Result from kmeans model for Use Case 01\").attach_feature_desc(inference_features_desc)\n",
    "   \n",
    "   fv_uc01_inference_result = fs.register_feature_view(\n",
    "         feature_view=fv_uc01_inference_result, \n",
    "         version= inf_fv_version, \n",
    "         block=True\n",
    "   )\n",
    "   print(f\"Inference Feature View : fv_uc01_inference_result_{inf_fv_version} created\")   \n",
    "else:\n",
    "   print(f\"Inference Feature View : fv_uc01_inference_result_{inf_fv_version} already created\")\n",
    "finally:\n",
    "   fs_serving_fviews = fs.list_feature_views().filter(F.col(\"NAME\") == inf_fvname ).sort(F.col(\"VERSION\").desc())\n",
    "   fs_serving_fviews.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureView(_name=FV_UC01_INFERENCE_RESULT, _entities=[Entity(name=CUSTOMER, join_keys=['O_CUSTOMER_SK'], owner=None, desc=Primary Key for CUSTOMER)], _feature_df=<snowflake.snowpark.dataframe.DataFrame object at 0x7f4a4a81f550>, _timestamp_col=None, _desc=Inference Result from kmeans model for Use Case 01, _query=SELECT \"O_CUSTOMER_SK\", \"FREQUENCY\", \"RETURN_RATIO\", \"LATEST_ORDER_DATE\",  CAST (\"TMP_RESULT\"['RETURN_RATIO_MMS'] AS DOUBLE) AS \"RETURN_RATIO_MMS\",  CAST (\"TMP_RESULT\"['FREQUENCY_MMS'] AS DOUBLE) AS \"FREQUENCY_MMS\",  CAST (\"TMP_RESULT\"['CLUSTER'] AS BIGINT) AS \"CLUSTER\" FROM (WITH SNOWPARK_ML_MODEL_INFERENCE_INPUT AS (SELECT * FROM TPCXAI_SF0001_QUICKSTART_INC._SERVING_FEATURE_STORE.FV_UC01_PREPROCESS$V_1),MODEL_VERSION_ALIAS AS MODEL TPCXAI_SF0001_QUICKSTART_INC._MODEL_REGISTRY.UC01_SNOWFLAKEML_KMEANS_MODEL VERSION V_1\n",
       "                SELECT *,\n",
       "                    MODEL_VERSION_ALIAS!PREDICT(RETURN_RATIO, FREQUENCY) AS TMP_RESULT\n",
       "                FROM SNOWPARK_ML_MODEL_INFERENCE_INPUT), _version=V_2, _status=FeatureViewStatus.STATIC, _feature_desc=OrderedDict([('FREQUENCY', 'Average yearly order frequency'), ('RETURN_RATIO', 'Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value'), ('LATEST_ORDER_DATE', ''), ('RETURN_RATIO_MMS', 'Min/Max Scaled version of RETURN_RATIO using Model Registry (TPCXAI_SF0001_QUICKSTART_INC_MODEL_REGISTRY) Model (UC01_SNOWFLAKEML_KMEANS_MODEL) Model-Version(V_1) Model Comment (TPCXAI USE CASE 01 - KMEANS - CUSTOMER PURCHASE CLUSTERS)'), ('FREQUENCY_MMS', 'Min/Max Scaled version of FREQUENCY using Model Registry (TPCXAI_SF0001_QUICKSTART_INC_MODEL_REGISTRY) Model (UC01_SNOWFLAKEML_KMEANS_MODEL) Model-Version(V_1)  Model Comment (TPCXAI USE CASE 01 - KMEANS - CUSTOMER PURCHASE CLUSTERS'), ('CLUSTER', 'Kmeans Cluster for Customer Clustering Model (UC01) using Model Registry (TPCXAI_SF0001_QUICKSTART_INC_MODEL_REGISTRY) Model (UC01_SNOWFLAKEML_KMEANS_MODEL) Model-Version(V_1)  Model Comment (TPCXAI USE CASE 01 - KMEANS - CUSTOMER PURCHASE CLUSTERS')]), _refresh_freq=None, _database=TPCXAI_SF0001_QUICKSTART_INC, _schema=_SERVING_FEATURE_STORE, _warehouse=None, _refresh_mode=None, _refresh_mode_reason=None, _owner=RAKESHGADIPARTHI)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fv_uc01_inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "|\"O_CUSTOMER_SK\"  |\"FREQUENCY\"  |\"RETURN_RATIO\"  |\"LATEST_ORDER_DATE\"  |\"RETURN_RATIO_MMS\"  |\"FREQUENCY_MMS\"  |\"CLUSTER\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "|                 |             |                |                     |                    |                 |           |\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fv_uc01_inference_result.feature_df.sort(F.col(\"LATEST_ORDER_DATE\").desc()).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
